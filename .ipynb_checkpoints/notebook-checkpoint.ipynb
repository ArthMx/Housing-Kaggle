{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from IPython.display import Image\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3435ddab-62ab-4c72-aa62-1497f7d265f9",
    "_uuid": "70a3fcaa0d170dc09945e706fa9d2dfa8730c1a0"
   },
   "source": [
    "# Stacking House Prices - Walkthrough to Top 5%\n",
    "\n",
    "### Arun Godwin Patel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d4ed9dce-931d-42ce-9286-9c0e41102f51",
    "_uuid": "37d903b57a6c6c4ecb7c25a587641ff0675a0496"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "25a169c3-1730-4f94-9ec9-21a80c82eb37",
    "_uuid": "c0f00831128f434f8c95423eb5b773792089a305",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(filename='../input/stacking-exp/stacking.gif.png', width = 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "39dfe4b9-5e88-4733-adde-b92a5551e06d",
    "_uuid": "84dce5761f3cf9550148a926df44ebdceb293f5c"
   },
   "source": [
    "You may be thinking... **\"What does a strange looking, 6 armed brick-layer have to do with predicting house prices?\"**\n",
    "\n",
    "Stick with it, all will be revealed throughout this walkthrough. But for now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ac8f67a7-7d90-492f-9284-244a76200b2f",
    "_uuid": "e9aa5df413c36df98d7c0c18c34427c539ca1e33"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dcadb7a3-96f1-4cff-b8ce-d4a508dd1f1f",
    "_uuid": "1061173cfd2f21a46563ac0fe75b479a8ba7b77f"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This kernel is intended to be a guide for education on the famous... Stacking technique. I used this technique to achieve a top 5% entry in the **House Prices: Advanced Regression Techniques** competition. I'll be focusing mainly on data preparation, feature engineering and the building of a stacking model. This is an ongoing project that I will update regularly, so stay tuned.\n",
    "\n",
    "If you have any comments, thoughts or notice anything that could be improved, please feel free to comment.\n",
    "\n",
    "Enjoy!\n",
    "\n",
    "First of all I like to do some research on the project at hand, in this case House Prices. So **what characteristics help to boost the value of a house?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "769bd108-a611-4349-8c3d-d84038b23928",
    "_uuid": "209ab7840ad6352f37a039c4915571e711d99d7b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(filename='../input/top10/top10.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b0769283-f835-4f47-bbec-22625b19d46d",
    "_uuid": "3eaa697509efae0502078db588229a944617c4eb"
   },
   "source": [
    "From this research, there are several key things that stood out:\n",
    "- **Location** - location is key for high valuations, therefore having a safe, well facilitated and well positioned house within a good neighbourhood, is also a large contributing factor.\n",
    "- **Size** - The more space, rooms and land that the house contains, the higher the valuation.\n",
    "- **Features** - the latest utilities and extras (such as a garage) ae highly desirable.\n",
    "\n",
    "This insight will held guide my feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9c488b63-881a-4e6b-a30f-88174a8dd92f",
    "_uuid": "c10e23d7976b7b97af4ec94ada192681544c9fc0"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "705e4fc7-81d6-482a-bd8c-d91851ee4de9",
    "_uuid": "a5250c2055f69ea5cad3e5318eebdaa93de797ca"
   },
   "source": [
    "## Content\n",
    "\n",
    "1. **Import packages**\n",
    "2. **Load data**\n",
    "3. **Data preparation**\n",
    "    - 3.1 - Remove outliers\n",
    "    - 3.2 - Target variable\n",
    "    - 3.3 - Treat missing values\n",
    "    - 3.4 - Transform numeric features into categorical features\n",
    "    - 3.5 - Label encoding\n",
    "4. **Feature engineering**\n",
    "    - 4.1 - Combinations\n",
    "    - 4.2 - Polynomials\n",
    "    - 4.3 - Treating skewed features\n",
    "5. **Modeling**\n",
    "6. **Conclusion**\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8ee13b9c-d07b-466e-b60b-598d96df60ed",
    "_uuid": "0a8c40d2bdc99ba535394ffaa8684156b2f85ab2"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "91f18325-c55c-49c4-b18a-df3422c0bee4",
    "_uuid": "4159a6dc100c4acfc4f33d6966ab60b12dd44120"
   },
   "source": [
    "# 1. \n",
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e2b4152d-d2bb-4b02-a200-1f9b6b246b16",
    "_uuid": "48a8dcf698d2dd034f4ccb06851d42d51983d0b7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This first set of packages include Pandas, for data manipulation, numpy for mathematical computation and matplotlib & seaborn, for visualisation.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set(style='white', context='notebook', palette='deep')\n",
    "print('Data Manipulation, Mathematical Computation and Visualisation packages imported!')\n",
    "\n",
    "# Statistical packages used for transformations\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, norm\n",
    "from scipy.special import boxcox1p\n",
    "print('Statistical packages imported!')\n",
    "\n",
    "# Metrics used for measuring the accuracy and performance of the models\n",
    "#from sklearn import metrics\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "print('Metrics packages imported!')\n",
    "\n",
    "# Algorithms used for modeling\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import xgboost as xgb\n",
    "print('Algorithm packages imported!')\n",
    "\n",
    "# Pipeline and scaling preprocessing will be used for models that are sensitive\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "print('Pipeline and preprocessing packages imported!')\n",
    "\n",
    "# Model selection packages used for sampling dataset and optimising parameters\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "print('Model selection packages imported!')\n",
    "\n",
    "# To ignore annoying warning\n",
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "print('Deprecation warning will be ignored!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "565063a6-e761-443c-817c-0b84875c6e4e",
    "_uuid": "c1c314bcb5f8cb74c6a24915de1c2fc29f9ed576"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b01474d2-6313-4380-bac4-4a13dfc67a7a",
    "_uuid": "0777cec7c00bc2bee154987ed713102c97ea8a38"
   },
   "source": [
    "# 2. \n",
    "## Load data\n",
    "\n",
    "- The Pandas package helps us work with our datasets. We start by reading the training and test datasets into DataFrames.\n",
    "- We want to save the 'Id' columns from both datasets for later use when preparing the submission data.\n",
    "- But we can drop them from the training and test datasets as they are redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0618a80c-d157-48cf-9865-91a4ca2cffe4",
    "_uuid": "aa60d4e812348ec28c3d0c8a7b8d81f8630873d6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\n",
    "test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n",
    "\n",
    "# Save the 'Id' column\n",
    "train_ID = train['Id']\n",
    "test_ID = test['Id']\n",
    "\n",
    "# Now drop the  'Id' column as it's redundant for modeling\n",
    "train.drop(\"Id\", axis = 1, inplace = True)\n",
    "test.drop(\"Id\", axis = 1, inplace = True)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "aeb45bf7-52c1-4c39-8251-bb05189582ab",
    "_uuid": "341739edb23487995b0a2977717729c5b3b3b32e"
   },
   "source": [
    "- This dataset was constructed by **Dean De Cock** for use in Data Science education. It is viewed as a modern alternative to the Boston Housing dataset.\n",
    "- As expressed within the competition, this datasets includes 79 descriptive features about the houses.\n",
    "- A data description is included within competition, I highly recommend referring to this file frequently during data preparation and feature engineering.\n",
    "- This file also gives guidance to how missing values should be treated, which I will address in section 3.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c69062f9-bda7-4745-84d8-23efd8fde121",
    "_uuid": "49057b6025a65538f97888210aebd3980e588713"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "42717325-a7ab-4677-bf2a-549444ed6f30",
    "_uuid": "24c819d76d9da953763a1e77f8290ae0e122d21f"
   },
   "source": [
    "# 3. \n",
    "## Data preparation\n",
    "### 3.1 - Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f99bfa21-a76e-49d9-bd21-1a853d308271",
    "_uuid": "04b99064da54b6f57a83e8cbb39fe6971ae1112a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(filename='../input/outliers/Outliers-Matter.jpg', width = 700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "47e983c4-5e96-47b5-a075-7c04533aa1bb",
    "_uuid": "0e1b0f8749487c98faa020af4b97430cbad375b6"
   },
   "source": [
    "***Outliers can be a Data Scientists nightmare.*** \n",
    "\n",
    "- By definition, an outlier is something that is outside of the expected response. How far you're willing to consider something to be an outlier, is down to the individual and the problem.\n",
    "- From this definition, this outlier will therefore sit way outside of the distribution of data points. Hence, this will skew the distribution of the data and potential calculations.\n",
    "- Let's see how this will affect predictions of the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "56e1743d-8fd0-4c10-b2c5-e93bc0a4ccd2",
    "_uuid": "4e209cfb79bb751d98ae7ff2fdc1a663051a94fa",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(filename='../input/outliers/outliers.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "68cf1598-99b0-49af-bea2-4c76de9a8d55",
    "_uuid": "447f60609427f11c46ef34cc8b76e6ba1ec9e4cf"
   },
   "source": [
    "The **data points** are shown in **light blue** on the left hand side of the grey dashed line. The **orange points** represent the **true future values**, and the **solid dark blue line** shows the **prediction** from the data points. \n",
    "\n",
    "- When the outliers are left in the model, the **model overfits** and is sensitive to these points. Therefore, it predicts values much higher than the true future values. *This is what we want to avoid.*\n",
    "- However, when outliers are removed, it **predicts much more accurately** with a generalised model that splits the distribution of the data points evenly.\n",
    "- ***This is very important in Machine Learning because our goal is to create robust models that are able to generalise to future situations.*** If we create a model that is very sensitive and tuned to fit outliers, this will result in a model that over or underfits. If we can create models that are able to cancel out the distractions and noise of outliers, this is usually a better situation.\n",
    "\n",
    "By referring to the **Ames Housing Dataset** link provided in the **Acknowledgements**, you'll see that the author outlines there are some outliers that must be treated: \n",
    "\n",
    "*\" Although all known errors were corrected in the data, no observations have been removed due to unusual values and all final residential sales from the initial data set are included in the data presented with this article. There are five observations that an instructor may wish to remove from the data set before giving it to students (a plot of SALE PRICE versus GR LIV AREA will quickly indicate these points). Three of them are true outliers (Partial Sales that likely don’t represent actual market values) and two of them are simply unusual sales (very large houses priced relatively appropriately). I would recommend removing any houses with more than 4000 square feet from the data set (which eliminates these five unusual observations) before assigning it to students. \"*\n",
    "\n",
    "- First, let's plot the two features stated against one another, to identify the outliers. Then we will remove them. The chart on the left shows the data before removing the outliers, and the chart on the right shows after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3b7755a0-b8d2-4125-8e4c-ec493eca38ea",
    "_uuid": "d3133622247fbeff94effd61075a8aa66cee6506",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "g = sns.regplot(x=train['GrLivArea'], y=train['SalePrice'], fit_reg=False).set_title(\"Before\")\n",
    "\n",
    "# Delete outliers\n",
    "plt.subplot(1, 2, 2)                                                                                \n",
    "train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n",
    "g = sns.regplot(x=train['GrLivArea'], y=train['SalePrice'], fit_reg=False).set_title(\"After\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3ba53cd0-3b36-43c3-9e40-e7be3d25264e",
    "_uuid": "ea93c377946930221a79bf2ea7d05d29520403a5"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "16710af8-dcce-4de1-a356-d990823c7ce2",
    "_uuid": "fb21fc0048ab015b4c919acc6f9be287eb54bd4a"
   },
   "source": [
    "### 3.2 - Target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d1ddc8db-d61d-4c7f-a767-5baf604749e0",
    "_uuid": "9529e27ca780b247b428b5ad027b774e15f1629e"
   },
   "source": [
    "- Unlike classification, **in regression we are predicting a continuous number**. Hence, the prediction could be any number along the real number line.\n",
    "- Therefore, it is always useful to check the distribution of the target variable, and indeed all numeric variables, when building a regression model. Machine Learning algorithms work well with features that are **normally distributed**, a distribution that is symmetric and has a characteristic bell shape. If features are not normally distributed, you can transform them using clever statistical methods.\n",
    "- First, let's check the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4ef62ac8-686a-4ac6-be3f-035922a82d58",
    "_uuid": "9a19350ab4b3df21554bf5480db384f0f68d7d05",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(15, 10))\n",
    "g = sns.distplot(train['SalePrice'], fit=norm, label = \"Skewness : %.2f\"%(train['SalePrice'].skew()));\n",
    "g = g.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fc5f985b-a7fe-4e3b-996c-b9c53046535d",
    "_uuid": "7f767df21cafb9a9f952f949a2331319defc898f"
   },
   "source": [
    "The distribution of the target variable is **positively skewed**, meaning that the mode is always less than the mean and median. \n",
    "\n",
    "- In order to transform this variable into a distribution that looks closer to the black line shown above, we can use the **numpy function log1p** which applies log(1+x) to all elements within the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8d45273b-bb17-4513-af55-cc0fc828d153",
    "_uuid": "6c847d892ef5243f08f47c331ad303246627c345",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\n",
    "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "\n",
    "#Check the new distribution \n",
    "plt.subplots(figsize=(15, 10))\n",
    "g = sns.distplot(train['SalePrice'], fit=norm, label = \"Skewness : %.2f\"%(train['SalePrice'].skew()));\n",
    "g = g.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bda73fd5-7566-4b94-afca-19c6d008e80c",
    "_uuid": "07feb93d5d8e2216373584568a1bc4a61474098e"
   },
   "source": [
    "We can see from the skewness and the plot that it follows much more closely to the normal distribution now. **This will help the algorithms work most reliably because we are now predicting a distribution that is well-known, i.e. the normal distribution**. If the distribution of your data approximates that of a theoretical distribution, we can perform calculations on the data that are based on assumptions of that well-known distribution. \n",
    "\n",
    "- ***Note:*** Now that we have transformed the target variable, this means that the prediction we produce will also be in the form of this transformation. Unless, we can revert this transformation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "14549c16-f672-46f7-bbd0-bd999c56464c",
    "_uuid": "7bd4ec747907a6cf8511f816cd40d8cf3d7738a1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(filename='../input/logexpo/loge.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6c5d1c95-17d8-4eb1-bb3b-8bcaad93c692",
    "_uuid": "0785d852c595e1a9aa565319ce7fbbeed3b02ddc"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8104b2f7-362d-4ae1-a1cc-40913d1b36b3",
    "_uuid": "ffe95bf1c68a536b644250956fd1d6b8a7081a41"
   },
   "source": [
    "### 3.3 - Treat missing values\n",
    "\n",
    "***Missing values are the Data Scientists other nightmare.***\n",
    "\n",
    "A missing value is an entry in a column that has no assigned value. This can mean multiple things:\n",
    "- A missing value may be the **result of an error during the production of the dataset**. This could be a human error, or machinery error depending on where the data comes from. \n",
    "- A missing value in some cases, may just mean a that a **'zero'** should be present. In which case, it can be replaced by a 0. The data description provided helps to address situations like these.\n",
    "- However, missing values represent no information. Therefore, **does the fact that you don't know what value to assign an entry, mean that filling it with a 'zero' is always a good fit?** \n",
    "\n",
    "Some algorithms do not like missing values. Some are capable of handling them, but others are not. Therefore since we are using a variety of algorithms, it's best to treat them in an appropriate way.\n",
    "\n",
    "**If you have missing values, you have two options**:\n",
    "- Delete the entire row\n",
    "- Fill the missing entry with an imputed value\n",
    "\n",
    "In order to treat this dataset, first of all I will create a dataset of the training and test data in order to make changes consistent across both. Then, I will cycle through each feature with missing values and treat them individually based on the data description, or my judgement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cd275e6c-7e56-440c-b90c-c4d17f17bf9d",
    "_uuid": "56c3329450ceeb4254fce759f1a664f86f620c0b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First of all, save the length of the training and test data for use later\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "\n",
    "# Also save the target value, as we will remove this\n",
    "y_train = train.SalePrice.values\n",
    "\n",
    "# concatenate training and test data into all_data\n",
    "all_data = pd.concat((train, test)).reset_index(drop=True)\n",
    "all_data.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "print(\"all_data shape: {}\".format(all_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b53b427a-fffe-47cf-8804-6f8583871eb2",
    "_uuid": "fcc88cd3dcf8ef9dd9d02e5294970fa307498fe8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# aggregate all null values \n",
    "all_data_na = all_data.isnull().sum()\n",
    "\n",
    "# get rid of all the values with 0 missing values\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\n",
    "plt.subplots(figsize =(15, 10))\n",
    "all_data_na.plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1291b8f-0850-4596-b26a-e4b844572a98",
    "_uuid": "7971f3f0431f06231368b404c5bebdb5b0495e15"
   },
   "source": [
    "Above you can see where the missing values sit. **Note** it may look like some of the features have 0 missing values, but actually they have 1 by closer inspection.\n",
    "\n",
    "- Through reference of the data description, this gives guidance on how to treat missing values for some columns. For ones where guidance isn't provided, I have used intuition which I will explain. Below, you will see how I treated each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4ef55726-a228-40a3-94a0-ffec1a18f8d0",
    "_uuid": "ced794fb74cecf651421b974de8f204b8e594106",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using data description, fill these missing values with \"None\"\n",
    "for col in (\"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\",\n",
    "           \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\",\n",
    "           \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\",\n",
    "            \"BsmtFinType2\", \"MSSubClass\", \"MasVnrType\"):\n",
    "    all_data[col] = all_data[col].fillna(\"None\")\n",
    "print(\"'None' - treated...\")\n",
    "\n",
    "# The area of the lot out front is likely to be similar to the houses in the local neighbourhood\n",
    "# Therefore, let's use the median value of the houses in the neighbourhood to fill this feature\n",
    "all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n",
    "    lambda x: x.fillna(x.median()))\n",
    "print(\"'LotFrontage' - treated...\")\n",
    "\n",
    "# Using data description, fill these missing values with 0 \n",
    "for col in (\"GarageYrBlt\", \"GarageArea\", \"GarageCars\", \"BsmtFinSF1\", \n",
    "           \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"MasVnrArea\",\n",
    "           \"BsmtFullBath\", \"BsmtHalfBath\"):\n",
    "    all_data[col] = all_data[col].fillna(0)\n",
    "print(\"'0' - treated...\")\n",
    "\n",
    "\n",
    "# Fill these features with their mode, the most commonly occuring value. This is okay since there are a low number of missing values for these features\n",
    "all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n",
    "all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n",
    "all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\n",
    "all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\n",
    "all_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\n",
    "all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n",
    "all_data[\"Functional\"] = all_data[\"Functional\"].fillna(all_data['Functional'].mode()[0])\n",
    "print(\"'mode' - treated...\")\n",
    "\n",
    "all_data_na = all_data.isnull().sum()\n",
    "print(\"Features with missing values: \", all_data_na.drop(all_data_na[all_data_na == 0].index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "be71039c-809b-4b2e-b62e-64e9c6814b6c",
    "_uuid": "fee07849db45902a056974c4d1ec47ee3ad81352"
   },
   "source": [
    "Here we see that we have 1 remaining feature with missing values, Utilities.\n",
    "\n",
    "- Let's inspect this closer to see how to treat it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3d6c3d3d-f497-4d89-bdd5-a25aa3eec8fc",
    "_uuid": "0d2bbc9184b72b7bddd0fd574857a4c82cf36cd0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize =(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "g = sns.countplot(x = \"Utilities\", data = train).set_title(\"Utilities - Training\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "g = sns.countplot(x = \"Utilities\", data = test).set_title(\"Utilities - Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "94cb720c-5cce-4550-be24-a0a82f67c839",
    "_uuid": "57013cce85214eab68aee42b14197478810a112b"
   },
   "source": [
    "This tell us that within the training dataset, Utilities has two unique values: \"AllPub\" and \"NoSeWa\". With \"AllPub\" being by far the most common.\n",
    "- However, the test dataset has only 1 value for this column, which means that it holds no predictive power because it is a constant for all test observations.\n",
    "\n",
    "Therefore, we can drop this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "272c2005-a4d5-4f93-b5b8-0bec0be8b5ac",
    "_uuid": "43338f474cbd216f110df64910c8b4a313d6ec5f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From inspection, we can remove Utilities\n",
    "all_data = all_data.drop(['Utilities'], axis=1)\n",
    "\n",
    "all_data_na = all_data.isnull().sum()\n",
    "print(\"Features with missing values: \", len(all_data_na.drop(all_data_na[all_data_na == 0].index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "24591c23-aa4d-4b2d-aac9-d1e27f962c4a",
    "_uuid": "711e3a8d3003a9731000e8442c5762dacf29260b"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0025f971-7f05-42f8-8827-9ac106802d7e",
    "_uuid": "fa6805d734fa5607a0a1f8d8fbfba521968ee1b8"
   },
   "source": [
    "### 3.4 - Transform numeric features into categorical features\n",
    "\n",
    "Some numeric columns are better off as categorical features. This may be because they represent a finite selection of numbers, which don't accurately represent a continuous number. Therefore, they are better of being represented as classes of a feature.\n",
    "\n",
    "- Continuous numbers can be encoded using binnings. These binnings have to be decided by the individual.\n",
    "- However, categorical features can be used to create dummy variables. With a finite number of possibilities, each value can be used to create its own variable. This is handy technique to know when feature engineering.\n",
    "\n",
    "Below are the features in the dataset that are better off as categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "420b5f09-7974-4395-92a9-87b92ba464b3",
    "_uuid": "024d13c50203c061e7186228e4674c4db8c869e9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MSSubClass = The building class\n",
    "all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n",
    "\n",
    "# Overall condition\n",
    "all_data['OverallCond'] = all_data['OverallCond'].astype(str)\n",
    "\n",
    "# Year and month sold\n",
    "all_data['YrSold'] = all_data['YrSold'].astype(str)\n",
    "all_data['MoSold'] = all_data['MoSold'].astype(str)\n",
    "\n",
    "# Garage built year - I would like to hand craft categoricl binnings. Therefore I will convert to an integer, replace the values with a category and then convert this to a more meaningful class \n",
    "all_data['GarageYrBlt'] = all_data['GarageYrBlt'].apply(int)\n",
    "\n",
    "# Locate and replace values\n",
    "all_data.loc[all_data['GarageYrBlt'] == 0, 'GarageYrBlt'] = 0\n",
    "all_data.loc[(all_data['GarageYrBlt'] >= 1) & (all_data['GarageYrBlt'] < 1900), 'GarageYrBlt'] = 1\n",
    "all_data.loc[(all_data['GarageYrBlt'] >= 1900) & (all_data['GarageYrBlt'] <= 1910), 'GarageYrBlt'] = 2\n",
    "all_data.loc[(all_data['GarageYrBlt'] >= 1911) & (all_data['GarageYrBlt'] <= 1920), 'GarageYrBlt'] = 3\n",
    "all_data.loc[(all_data['GarageYrBlt'] >= 1921) & (all_data['GarageYrBlt'] <= 1930), 'GarageYrBlt'] = 4\n",
    "all_data.loc[(all_data['GarageYrBlt'] >= 1931) & (all_data['GarageYrBlt'] <= 1940), 'GarageYrBlt'] = 5\n",
    "all_data.loc[(all_data['GarageYrBlt'] >= 1941) & (all_data['GarageYrBlt'] <= 1950), 'GarageYrBlt'] = 6\n",
    "all_data.loc[(all_data['GarageYrBlt'] >= 1951) & (all_data['GarageYrBlt'] <= 1960), 'GarageYrBlt'] = 7\n",
    "all_data.loc[(all_data['GarageYrBlt'] >= 1961) & (all_data['GarageYrBlt'] <= 1970), 'GarageYrBlt'] = 8\n",
    "all_data.loc[(all_data['GarageYrBlt'] >= 1971) & (all_data['GarageYrBlt'] <= 1980), 'GarageYrBlt'] = 9\n",
    "all_data.loc[(all_data['GarageYrBlt'] >= 1981) & (all_data['GarageYrBlt'] <= 1990), 'GarageYrBlt'] = 10\n",
    "all_data.loc[(all_data['GarageYrBlt'] >= 1991) & (all_data['GarageYrBlt'] <= 2000), 'GarageYrBlt'] = 11\n",
    "all_data.loc[(all_data['GarageYrBlt'] >= 2001) & (all_data['GarageYrBlt'] <= 2010), 'GarageYrBlt'] = 12\n",
    "all_data.loc[(all_data['GarageYrBlt'] >= 2011), 'GarageYrBlt'] = 13\n",
    "\n",
    "# Replace with categorical values\n",
    "all_data['GarageYrBlt'] = all_data['GarageYrBlt'].replace(0, 'NoGarage')\n",
    "all_data['GarageYrBlt'] = all_data['GarageYrBlt'].replace(1, 'less_1900')\n",
    "all_data['GarageYrBlt'] = all_data['GarageYrBlt'].replace(2, '1900_1910')\n",
    "all_data['GarageYrBlt'] = all_data['GarageYrBlt'].replace(3, '1911_1920')\n",
    "all_data['GarageYrBlt'] = all_data['GarageYrBlt'].replace(4, '1921_1930')\n",
    "all_data['GarageYrBlt'] = all_data['GarageYrBlt'].replace(5, '1931_1940')\n",
    "all_data['GarageYrBlt'] = all_data['GarageYrBlt'].replace(6, '1941_1950')\n",
    "all_data['GarageYrBlt'] = all_data['GarageYrBlt'].replace(7, '1951_1960')\n",
    "all_data['GarageYrBlt'] = all_data['GarageYrBlt'].replace(8, '1961_1970')\n",
    "all_data['GarageYrBlt'] = all_data['GarageYrBlt'].replace(9, '1971_1980')\n",
    "all_data['GarageYrBlt'] = all_data['GarageYrBlt'].replace(10, '1981_1990')\n",
    "all_data['GarageYrBlt'] = all_data['GarageYrBlt'].replace(11, '1991_2000')\n",
    "all_data['GarageYrBlt'] = all_data['GarageYrBlt'].replace(12, '2001_2010')\n",
    "all_data['GarageYrBlt'] = all_data['GarageYrBlt'].replace(13, '2010+')\n",
    "\n",
    "print(\"Transformations of numerical to categorical features = complete...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5381ddba-9682-4987-9e19-b79d0257d711",
    "_uuid": "4791b869ab1b8ebffd2f6d8ccd0eabd2d84ca61d"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "83579d5f-c1fa-4905-8132-bcf6b846eaf5",
    "_uuid": "1448bde54ef901692e316df0da9062d0ad1c32c8"
   },
   "source": [
    "### 3.5 - Label encoding\n",
    "\n",
    "Algorithms work best with numerical values. Therefore, **what can we do with features that include strings as their values?**\n",
    "\n",
    "- Well for each unique string contained in a feature, we can replace them with a unique numerical value.\n",
    "- We can do this, using the **Label Encoder**, that does this automatically for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "13733967-f441-4eca-b145-9912e4e93541",
    "_uuid": "fbe8b7e2343e83cce0b6994e7d26655d5bf2df82",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n",
    "        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n",
    "        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n",
    "        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n",
    "        'YrSold', 'MoSold')\n",
    "# process columns, apply LabelEncoder to categorical features\n",
    "for c in columns:\n",
    "    lbl = LabelEncoder() \n",
    "    lbl.fit(list(all_data[c].values)) \n",
    "    all_data[c] = lbl.transform(list(all_data[c].values))\n",
    "        \n",
    "print('Shape all_data: ', all_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6b8705a8-7942-49c0-bf82-14d6e9737e1e",
    "_uuid": "d781e9b6f870642ced39aa6375b8111d694b0d9d"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0ff2fc29-f692-48bf-b3e1-b5c8d50d9de7",
    "_uuid": "8e52ffbfaf1330de9c2abe7b639cf0279afb8263"
   },
   "source": [
    "## 4. Feature engineering\n",
    "### 4.1 - Combinations\n",
    "\n",
    "Using the 79 descriptive variables that we have in the dataset, we can combine some of them together to create new variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d37f6f82-a838-4d50-9ae1-3ece06f7b204",
    "_uuid": "8058acf4fde3230ce8c66e4377dbfd3b49310d44",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Overall quality of the house\n",
    "all_data[\"OverallGrade\"] = all_data[\"OverallQual\"] * all_data[\"OverallCond\"]\n",
    "# Overall quality of the garage\n",
    "all_data[\"GarageGrade\"] = all_data[\"GarageQual\"] * all_data[\"GarageCond\"]\n",
    "# Overall quality of the exterior\n",
    "all_data[\"ExterGrade\"] = all_data[\"ExterQual\"] * all_data[\"ExterCond\"]\n",
    "# Overall kitchen score\n",
    "all_data[\"KitchenScore\"] = all_data[\"KitchenAbvGr\"] * all_data[\"KitchenQual\"]\n",
    "# Overall fireplace score\n",
    "all_data[\"FireplaceScore\"] = all_data[\"Fireplaces\"] * all_data[\"FireplaceQu\"]\n",
    "# Overall garage score\n",
    "all_data[\"GarageScore\"] = all_data[\"GarageArea\"] * all_data[\"GarageQual\"]\n",
    "# Overall pool score\n",
    "all_data[\"PoolScore\"] = all_data[\"PoolArea\"] * all_data[\"PoolQC\"]\n",
    "# Overall quality of the house\n",
    "all_data[\"OverallGrade\"] = all_data[\"OverallQual\"] * all_data[\"OverallCond\"]\n",
    "# Overall quality of the exterior\n",
    "all_data[\"OverallExterGrade\"] = all_data[\"ExterQual\"] * all_data[\"ExterCond\"]\n",
    "# Overall pool score\n",
    "all_data[\"OverallPoolScore\"] = all_data[\"PoolArea\"] * all_data[\"PoolQC\"]\n",
    "# Overall garage score\n",
    "all_data[\"OverallGarageScore\"] = all_data[\"GarageArea\"] * all_data[\"GarageQual\"]\n",
    "# Overall fireplace score\n",
    "all_data[\"OverallFireplaceScore\"] = all_data[\"Fireplaces\"] * all_data[\"FireplaceQu\"]\n",
    "# Total number of bathrooms\n",
    "all_data[\"TotalBath\"] = all_data[\"BsmtFullBath\"] + (0.5 * all_data[\"BsmtHalfBath\"]) + all_data[\"FullBath\"] + (0.5 * all_data[\"HalfBath\"])\n",
    "# Total SF for house\n",
    "all_data[\"TotalSF\"] = all_data[\"GrLivArea\"] + all_data[\"TotalBsmtSF\"]\n",
    "# Total SF for 1st + 2nd floors\n",
    "all_data[\"FloorsSF\"] = all_data[\"1stFlrSF\"] + all_data[\"2ndFlrSF\"]\n",
    "# Total SF for porch\n",
    "all_data[\"PorchSF\"] = all_data[\"OpenPorchSF\"] + all_data[\"EnclosedPorch\"] + all_data[\"3SsnPorch\"] + all_data[\"ScreenPorch\"]\n",
    "# Has masonry veneer or not\n",
    "all_data[\"HasMasVnr\"] = all_data.MasVnrType.replace({\"BrkCmn\" : 1, \"BrkFace\" : 1, \"CBlock\" : 1, \"Stone\" : 1, \"None\" : 0})\n",
    "# House completed before sale or not\n",
    "all_data[\"CompletedBFSale\"] = all_data.SaleCondition.replace({\"Abnorml\" : 0, \"Alloca\" : 0, \"AdjLand\" : 0, \"Family\" : 0, \"Normal\" : 0, \"Partial\" : 1})\n",
    "print(\"Combinations completed...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "002a28f9-5c97-40e7-804b-599435d82f7a",
    "_uuid": "591384a89b60e14fb28f9d49e7bf2fd613291196"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1e6835d3-8faf-4f14-8a8a-a79fcc34c911",
    "_uuid": "01679b7f635cb223df4acb52bb15d2e7cf286a61"
   },
   "source": [
    "### 4.2 - Polynomials\n",
    "\n",
    "The most common relationship we may think of between two variables, would be a straight line or a linear relationship. What this means is that if we increase the predictor by 1 unit, the response always increases by X units. However, not all data has a linear relationship and therefore it may be necessary for your model to fit the more complex relationships in the data. \n",
    "\n",
    "But how do you fit a model to data with complex relationships, unexplainable by a linear function? There are a variety of curve-fitting methods you can choose from to help you with this.\n",
    "\n",
    "- The most common way to fit curves to the data is to include polynomial terms, such as squared or cubed predictors.\n",
    "- Typically, you choose the model order by the number of bends you need in your line. Each increase in the exponent produces one more bend in the curved fitted line. It’s very rare to use more than a cubic term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "94f6a1f9-698b-461f-85bc-69cdb51461e8",
    "_uuid": "5b38ee7a7b3642f0ad1ec3482b1481924fe0ca10",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(filename='../input/combinations/combinations.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dc372602-5308-460e-8ea9-5a13cddb4173",
    "_uuid": "d3263080cddc8f5a5f46ca19fda2cbeaac43829d"
   },
   "source": [
    "- If your response data follows a pattern that descends down to a lower bound, or ascends up to an upper bound, you can fit this type of relationship by including the reciprocal (1/x) of one or more predictor variables in the model.\n",
    "    - Generally, you want to use this form when the size of effect for a predictor variable decreases as its value increases. \n",
    "- Because the gradient is a function of 1/x, the gradient gets flatter as x increases. For this type of model, x can never equal 0 because you can’t divide by zero.\n",
    "\n",
    "*So... now that you're armed with this information, what's important to know is that in order to model non-linear, complex relationships between response and predictor variables, we can create combinations of these variables with increasing order, or reciprocal orders.*\n",
    "\n",
    "- Since we have such a high number of variables in the dataset, it's overkill to create polynomials of each feature. Therefore, I will look at the top 10 correlating features with the target variable from the training dataset and create polynomials of these features, or equivalently the new combinations I have created from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "87b63e61-7e35-4ee6-9ecb-ba439904cafd",
    "_uuid": "d7c8093be0792916c8bd769431b452dd785a74ae",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# correlations with the target\n",
    "corr = train.corr()\n",
    "corr.sort_values([\"SalePrice\"], ascending = True, inplace = True)\n",
    "corr = corr.SalePrice\n",
    "plt.subplots(figsize =(15, 10))\n",
    "corr.plot(kind='barh');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "de9583bf-c34e-4ba9-8133-31e1571ab7c3",
    "_uuid": "a9373394cf12e17482ad1ab7ed7353176ffdd6b7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 features polynomials\n",
    "\n",
    "# Quadratic\n",
    "all_data[\"OverallQual-2\"] = all_data[\"OverallQual\"] ** 2\n",
    "all_data[\"TotalSF-2\"] = all_data[\"TotalSF\"] ** 2\n",
    "all_data[\"FloorsSF-2\"] = all_data[\"FloorsSF\"] ** 2\n",
    "all_data[\"GrLivArea-2\"] = all_data[\"GrLivArea\"] ** 2\n",
    "all_data[\"ExterQual-2\"] = all_data[\"ExterQual\"] ** 2\n",
    "all_data[\"GarageCars-2\"] = all_data[\"GarageCars\"] ** 2\n",
    "all_data[\"TotalBath-2\"] = all_data[\"TotalBath\"] ** 2\n",
    "all_data[\"KitchenQual-2\"] = all_data[\"KitchenQual\"] ** 2\n",
    "all_data[\"GarageScore-2\"] = all_data[\"GarageScore\"] ** 2\n",
    "print(\"Quadratics done...\")\n",
    "\n",
    "# Cubic\n",
    "all_data[\"OverallQual-3\"] = all_data[\"OverallQual\"] ** 3\n",
    "all_data[\"TotalSF-3\"] = all_data[\"TotalSF\"] ** 3\n",
    "all_data[\"FloorsSF-3\"] = all_data[\"FloorsSF\"] ** 3\n",
    "all_data[\"GrLivArea-3\"] = all_data[\"GrLivArea\"] ** 3\n",
    "all_data[\"ExterQual-3\"] = all_data[\"ExterQual\"] ** 3\n",
    "all_data[\"GarageCars-3\"] = all_data[\"GarageCars\"] ** 3\n",
    "all_data[\"TotalBath-3\"] = all_data[\"TotalBath\"] ** 3\n",
    "all_data[\"KitchenQual-3\"] = all_data[\"KitchenQual\"] ** 3\n",
    "all_data[\"GarageScore-3\"] = all_data[\"GarageScore\"] ** 3\n",
    "print(\"Cubics done...\")\n",
    "\n",
    "# Square Root\n",
    "all_data[\"OverallQual-Sq\"] = np.sqrt(all_data[\"OverallQual\"])\n",
    "all_data[\"TotalSF-Sq\"] = np.sqrt(all_data[\"TotalSF\"])\n",
    "all_data[\"FloorsSF-Sq\"] = np.sqrt(all_data[\"FloorsSF\"])\n",
    "all_data[\"GrLivArea-Sq\"] = np.sqrt(all_data[\"GrLivArea\"])\n",
    "all_data[\"ExterQual-Sq\"] = np.sqrt(all_data[\"ExterQual\"])\n",
    "all_data[\"GarageCars-Sq\"] = np.sqrt(all_data[\"GarageCars\"])\n",
    "all_data[\"TotalBath-Sq\"] = np.sqrt(all_data[\"TotalBath\"])\n",
    "all_data[\"KitchenQual-Sq\"] = np.sqrt(all_data[\"KitchenQual\"])\n",
    "all_data[\"GarageScore-Sq\"] = np.sqrt(all_data[\"GarageScore\"])\n",
    "print(\"Square Roots done...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6f92ecd4-0879-40e8-971b-1ec0e81238cc",
    "_uuid": "8737c6b16ac65984466d2b1ea0e8bdc477a37c69"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ac8103db-76ff-4633-a877-8ed651d1ff40",
    "_uuid": "620bf48102bbf6b6e60c1c8d77690aa3cfe16b48"
   },
   "source": [
    "### 4.3 - Treating skewed features\n",
    "\n",
    "As touched on earlier, skewed numeric variables are not desirable when using Machine Learning algorithms. The reason why we want to do this is move the models focus away from any extreme values, to create a generalised solution. We can tame these extreme values by transforming skewed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6f6d7d29-dc91-454c-8121-94d208ef5b93",
    "_uuid": "5e95ff1c813fba9226e57666205b82ecb8ce39f7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First lets single out the numeric features\n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "\n",
    "# Check how skewed they are\n",
    "skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "\n",
    "plt.subplots(figsize =(15, 25))\n",
    "skewed_feats.plot(kind='barh');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "63bea335-64ea-40d7-ab90-eaa7418f31a0",
    "_uuid": "aaed75847e6c9d4fd0f17759c29908edbacc6f5a"
   },
   "source": [
    "Clearly, we have a variety of positive and negative skewing features. Now I will transform the features with skew > 0.5 to follow more closely the normal distribution.\n",
    "\n",
    "- **Note**: I am using the Box-Cox transformation to transform non-normal variables into a normal shape. Normality is an important assumption for many statistical techniques; if your data isn't normal, applying a Box-Cox means that you are able to run a broader number of tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f16ff324-c2bb-4dbf-ba55-43edf2e911be",
    "_uuid": "34d73717ef8f8efbfecf58f5a9b4a98d7472ee57",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skewness = skewed_feats[abs(skewed_feats) > 0.5]\n",
    "print(\"There are \", skewness.shape[0],  \"skewed numerical features to Box-Cox transform\")\n",
    "\n",
    "skewed_features = skewness.index\n",
    "lam = 0.15\n",
    "for feat in skewed_features:\n",
    "    all_data[feat] = boxcox1p(all_data[feat], lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d7e80954-a5e5-40bc-9816-21987fba9902",
    "_uuid": "03048750adf9e7c39f334cfc2cb2ad04c8ef6437"
   },
   "source": [
    "- Our data is almost ready for modeling. The last thing we must do is engineer dummy variables from all of the categorical features we have in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "69ae4b0e-ef85-426e-9725-563e31d2a37d",
    "_uuid": "9c2cbf69dae4b24e883742b708f846d84ec535ad",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create final dummies\n",
    "all_data = pd.get_dummies(all_data)\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "411848dc-0721-488b-bbed-b8c2ad69a3dd",
    "_uuid": "a5527610a0989dc490c7677c66c0c5519ae8cd86"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7e87f0c7-d932-4ecd-ae3f-c9fdc3952475",
    "_uuid": "cb12d23db4212f1e5c4075eb58d8c9afcada7b51"
   },
   "source": [
    "# 5. \n",
    "## Modeling\n",
    "### 5.1 - Preparation of data\n",
    "\n",
    "- Now that our dataset is ready for modeling, we must prepare it from training, testing and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c7a361c1-ba67-49e3-b753-725347d101ba",
    "_uuid": "e89d1c701a417a09e16f88c33c5d275f6fc85b11",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, re-create the training and test datasets\n",
    "train = all_data[:ntrain]\n",
    "test = all_data[ntrain:]\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bbd48f2b-bf67-47e4-8a38-48c23d3688ee",
    "_uuid": "14a61179424e950213f4d5d475d6a6831e342bdd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next we want to sample our training data to test for performance of robustness ans accuracy, before applying to the test data\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(train, y_train, test_size=0.3, random_state=42)\n",
    "\n",
    "# X_train = predictor features for estimation dataset\n",
    "# X_test = predictor variables for validation dataset\n",
    "# Y_train = target variable for the estimation dataset\n",
    "# Y_test = target variable for the estimation dataset\n",
    "\n",
    "print('X_train: ', X_train.shape, '\\nX_test: ', X_test.shape, '\\nY_train: ', Y_train.shape, '\\nY_test: ', Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "76f9d649-c96e-48ae-bdd1-3300eae82727",
    "_uuid": "6f5a9b36ed2d5aa539640f7d4e4b9cf05980891b"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "52d2160f-c409-4ef9-bc1b-81bac61ca528",
    "_uuid": "8505ffb41ee328136ee61ea69d72c26b5f0d4ab4"
   },
   "source": [
    "### 5.2 - Training\n",
    "\n",
    "We are finally ready to train our models. For this analysis I am using 8 different algorithms:\n",
    "- **Kernel Ridge Regression**\n",
    "- **Elastic Net**\n",
    "- **Lasso**\n",
    "- **Gradient Boosting**\n",
    "- **Bayesian Ridge**\n",
    "- **Lasso Lars IC**\n",
    "- **Random Forest Regressor**\n",
    "- **XGBoost**\n",
    "\n",
    "The method of measuring accuracy was chosen to be **Root Mean Squared Error**, as described within the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f06def91-4f4a-499c-9e4e-59edb14901fe",
    "_uuid": "413d60cd80f41ee1453ecde29ac7f43793241f1b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Machine Learning Algorithm (MLA) Selection and Initialization\n",
    "models = [KernelRidge(), ElasticNet(), Lasso(), GradientBoostingRegressor(), BayesianRidge(), LassoLarsIC(), RandomForestRegressor(), xgb.XGBRegressor()]\n",
    "\n",
    "# First I will use ShuffleSplit as a way of randomising the cross validation samples.\n",
    "shuff = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "\n",
    "#create table to compare MLA metrics\n",
    "columns = ['Name', 'Parameters', 'Train Accuracy Mean', 'Test Accuracy']\n",
    "before_model_compare = pd.DataFrame(columns = columns)\n",
    "\n",
    "#index through models and save performance to table\n",
    "row_index = 0\n",
    "for alg in models:\n",
    "\n",
    "    #set name and parameters\n",
    "    model_name = alg.__class__.__name__\n",
    "    before_model_compare.loc[row_index, 'Name'] = model_name\n",
    "    before_model_compare.loc[row_index, 'Parameters'] = str(alg.get_params())\n",
    "    \n",
    "    alg.fit(X_train, Y_train)\n",
    "    \n",
    "    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n",
    "    training_results = np.sqrt((-cross_val_score(alg, X_train, Y_train, cv = shuff, scoring= 'neg_mean_squared_error')).mean())\n",
    "    test_results = np.sqrt(((Y_test-alg.predict(X_test))**2).mean())\n",
    "    \n",
    "    before_model_compare.loc[row_index, 'Train Accuracy Mean'] = (training_results)*100\n",
    "    before_model_compare.loc[row_index, 'Test Accuracy'] = (test_results)*100\n",
    "    \n",
    "    row_index+=1\n",
    "    print(row_index, alg.__class__.__name__, 'trained...')\n",
    "\n",
    "decimals = 3\n",
    "before_model_compare['Train Accuracy Mean'] = before_model_compare['Train Accuracy Mean'].apply(lambda x: round(x, decimals))\n",
    "before_model_compare['Test Accuracy'] = before_model_compare['Test Accuracy'].apply(lambda x: round(x, decimals))\n",
    "before_model_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5b968c6c-ecc4-44cf-b8ab-57428eba13fc",
    "_uuid": "dc59c17eda8ea78a8de0bba21693086d357e2a2a"
   },
   "source": [
    "- We can see that each of the models performs with varying ability, with **Bayesian Ridge** having the best accuracy score on the training dataset and accuracy on the validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "de3bca75-3f4c-4e25-9867-66bd49f8ca49",
    "_uuid": "5233e96eaf5fc259fb4f537e7fb701d129c5cdb4"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e0b7ccf2-3658-4f39-867f-3fd33dfa0514",
    "_uuid": "d3d0e631e5948520e4ceea0e5043fbe7ddfc2c35"
   },
   "source": [
    "### 5.3 - Optimisation\n",
    "\n",
    "- As you can see from the above table, the accuracy for these models is not quite as good as it could be.\n",
    "- This is because we use the default configuration of parameters for each of the algorithms.\n",
    "\n",
    "So now, we will use **GridSearchCV** to find the best combinations of parameters to produce the highest scoring models.\n",
    "\n",
    "**Note**: GridSearchCV uses a grid of parameters to optimise the algorithms. This grid can get extremely large, and therefore requires a lot of computation power to complete. I have included a set of answers in the grids to cut down computation time, but these were not my final ones. I'll leave this up to you to find the best values. But in reality, you will have to fill these grids with appropriate values with the goal of trying to find the best combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9a6b0a9a-aeed-40cb-9206-7cf9156bb8a6",
    "_uuid": "fa5a1001ff202dd0d5d28877dddd6ce851ce235d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [KernelRidge(), ElasticNet(), Lasso(), GradientBoostingRegressor(), BayesianRidge(), LassoLarsIC(), RandomForestRegressor(), xgb.XGBRegressor()]\n",
    "\n",
    "KR_param_grid = {'alpha': [0.1], 'coef0': [100], 'degree': [1], 'gamma': [None], 'kernel': ['polynomial']}\n",
    "EN_param_grid = {'alpha': [0.001], 'copy_X': [True], 'l1_ratio': [0.6], 'fit_intercept': [True], 'normalize': [False], \n",
    "                         'precompute': [False], 'max_iter': [300], 'tol': [0.001], 'selection': ['random'], 'random_state': [None]}\n",
    "LASS_param_grid = {'alpha': [0.0005], 'copy_X': [True], 'fit_intercept': [True], 'normalize': [False], 'precompute': [False], \n",
    "                    'max_iter': [300], 'tol': [0.01], 'selection': ['random'], 'random_state': [None]}\n",
    "GB_param_grid = {'loss': ['huber'], 'learning_rate': [0.1], 'n_estimators': [300], 'max_depth': [3], \n",
    "                                        'min_samples_split': [0.0025], 'min_samples_leaf': [5]}\n",
    "BR_param_grid = {'n_iter': [200], 'tol': [0.00001], 'alpha_1': [0.00000001], 'alpha_2': [0.000005], 'lambda_1': [0.000005], \n",
    "                 'lambda_2': [0.00000001], 'copy_X': [True]}\n",
    "LL_param_grid = {'criterion': ['aic'], 'normalize': [True], 'max_iter': [100], 'copy_X': [True], 'precompute': ['auto'], 'eps': [0.000001]}\n",
    "RFR_param_grid = {'n_estimators': [50], 'max_features': ['auto'], 'max_depth': [None], 'min_samples_split': [5], 'min_samples_leaf': [2]}\n",
    "XGB_param_grid = {'max_depth': [3], 'learning_rate': [0.1], 'n_estimators': [300], 'booster': ['gbtree'], 'gamma': [0], 'reg_alpha': [0.1],\n",
    "                  'reg_lambda': [0.7], 'max_delta_step': [0], 'min_child_weight': [1], 'colsample_bytree': [0.5], 'colsample_bylevel': [0.2],\n",
    "                  'scale_pos_weight': [1]}\n",
    "params_grid = [KR_param_grid, EN_param_grid, LASS_param_grid, GB_param_grid, BR_param_grid, LL_param_grid, RFR_param_grid, XGB_param_grid]\n",
    "\n",
    "after_model_compare = pd.DataFrame(columns = columns)\n",
    "\n",
    "row_index = 0\n",
    "for alg in models:\n",
    "    \n",
    "    gs_alg = GridSearchCV(alg, param_grid = params_grid[0], cv = shuff, scoring = 'neg_mean_squared_error', n_jobs=-1)\n",
    "    params_grid.pop(0)\n",
    "\n",
    "    #set name and parameters\n",
    "    model_name = alg.__class__.__name__\n",
    "    after_model_compare.loc[row_index, 'Name'] = model_name\n",
    "    \n",
    "    gs_alg.fit(X_train, Y_train)\n",
    "    gs_best = gs_alg.best_estimator_\n",
    "    after_model_compare.loc[row_index, 'Parameters'] = str(gs_alg.best_params_)\n",
    "    \n",
    "    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n",
    "    after_training_results = np.sqrt(-gs_alg.best_score_)\n",
    "    after_test_results = np.sqrt(((Y_test-gs_alg.predict(X_test))**2).mean())\n",
    "    \n",
    "    after_model_compare.loc[row_index, 'Train Accuracy Mean'] = (after_training_results)*100\n",
    "    after_model_compare.loc[row_index, 'Test Accuracy'] = (after_test_results)*100\n",
    "    \n",
    "    row_index+=1\n",
    "    print(row_index, alg.__class__.__name__, 'trained...')\n",
    "\n",
    "decimals = 3\n",
    "after_model_compare['Train Accuracy Mean'] = after_model_compare['Train Accuracy Mean'].apply(lambda x: round(x, decimals))\n",
    "after_model_compare['Test Accuracy'] = after_model_compare['Test Accuracy'].apply(lambda x: round(x, decimals))\n",
    "after_model_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "30584443-52f1-40b9-9900-1e249ba87c20",
    "_uuid": "a408375d36493f1ecca5b3bfa33114e80e2d8cec"
   },
   "source": [
    "Overall we can see that the training and test scores for each of the models have decreased, which is what we want.\n",
    "- Now we have a set of highly tuned algorithms to use for **Stacking**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "be78a481-7158-4997-aad9-42f600225bb9",
    "_uuid": "b6555fdc369881765562562429cfc0d4c40d39cf"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bcf8b892-ff40-412d-b9da-a6d66118a7b8",
    "_uuid": "fe27c88a08dc6fc41535e863c7ad7e1c44a6b1eb"
   },
   "source": [
    "### 5.4 - Stacking\n",
    "\n",
    "Now that we have a set of highly tuned algorithms, a rather famous and successful technique to further improve the accuracy of these models, is to use **Stacking**. Let me explain what this means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2a165607-9274-411d-80ce-5edc58b03eb5",
    "_uuid": "0f35d343b74ccf485f24dc9c87cf7f8b4b437f36",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Image(filename='../input/stacking-exp/stacking.gif.png', width = 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "21019d9f-b55a-4eb6-b137-e76b0586fee9",
    "_uuid": "652b6d1d0cdcd0925dc3ca5121937aef0057ee88"
   },
   "source": [
    "If you hadn't figured it out already, our brick-laying friend featured earlier to introduce the notion of  **Stacking**.\n",
    "\n",
    "Brick-laying is an art form. Where I live in London today, there remain buildings that have stood for hundreds and even thousands of years. Without a skilled brick-layer to stack them properly, nobody would ever want to visit or live in this city. This animation shows the art of stacking bricks on top of one another to form something much greater, a wall, a house or even a building. **This is exactly what we are going to do by stacking several algorithms together, to form a much stronger one.**  \n",
    "\n",
    "The steps for this technique are shown below:\n",
    "1. **Create a set of algorithms ready for stacking** - We've done this...\n",
    "2. **Split the original training data into a training and validation sample** - We've done this too...\n",
    "3. **Train the algorithms on the training sample** - Also done this...\n",
    "4. **For each algorithm, apply the trained models to the validation dataset and create a set of predictions**, 1 column for each model, as a new table. Call this the *new training dataset*.\n",
    "5. **Also apply the trained algorithm to the test dataset and create a final set of predictions**, 1 column for each model, as a new table. Call this *new test dataset*.\n",
    "6. **For the new training dataset, we have labeled outputs, in the form of Y_test**. Now we must train another model on these two feature sets: *new training dataset* and Y_test.\n",
    "7. **Use this newly trained model to predict values** for *new test dataset*.\n",
    "\n",
    "Now I understand that this sounds very confusing, and probably doesn't make much sense. Let me explain this further with some visualisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dd6877de-3f10-4703-b45d-3758da000ee3",
    "_uuid": "20f513f04d3b160a33d7e36aa00b666bf2a2056e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(filename='../input/stacking-exp/stackingexp.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3e0a9a1c-92f3-4ca3-8a97-f386c743e865",
    "_uuid": "8540d502af6bc1d8253589484c3a318a87f397f8"
   },
   "source": [
    "- Before I start with the stacking, I need to decide which algorithms to use as my base estimators, and which to use as the meta-model.\n",
    "\n",
    "- Since **Lasso** performed the best after optimisation, I chose this to be the **meta-model**. All other models will be used as base estimators.\n",
    "\n",
    "- So now, I will cycle through each optimised estimator, train them on the training dataset, apply to them the validation and test datasets, then finally outputting the predictions for validation and test into two new datasets: **stacked_validation_train** and **stacked_test_train**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "023c86f8-11f7-4e35-b5c5-ffe23a4e718e",
    "_uuid": "7a93576c725ffc4ec250f5ea4a37d5c10a36f403",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [KernelRidge(), ElasticNet(), Lasso(), GradientBoostingRegressor(), BayesianRidge(), LassoLarsIC(), RandomForestRegressor(), xgb.XGBRegressor()]\n",
    "names = ['KernelRidge', 'ElasticNet', 'Lasso', 'Gradient Boosting', 'Bayesian Ridge', 'Lasso Lars IC', 'Random Forest', 'XGBoost']\n",
    "params_grid = [KR_param_grid, EN_param_grid, LASS_param_grid, GB_param_grid, BR_param_grid, LL_param_grid, RFR_param_grid, XGB_param_grid]\n",
    "stacked_validation_train = pd.DataFrame()\n",
    "stacked_test_train = pd.DataFrame()\n",
    "\n",
    "row_index=0\n",
    "\n",
    "for alg in models:\n",
    "    \n",
    "    gs_alg = GridSearchCV(alg, param_grid = params_grid[0], cv = shuff, scoring = 'neg_mean_squared_error', n_jobs=-1)\n",
    "    params_grid.pop(0)\n",
    "    \n",
    "    gs_alg.fit(X_train, Y_train)\n",
    "    gs_best = gs_alg.best_estimator_\n",
    "    stacked_validation_train.insert(loc = row_index, column = names[0], value = gs_best.predict(X_test))\n",
    "    print(row_index+1, alg.__class__.__name__, 'predictions added to stacking validation dataset...')\n",
    "    \n",
    "    stacked_test_train.insert(loc = row_index, column = names[0], value = gs_best.predict(test))\n",
    "    print(row_index+1, alg.__class__.__name__, 'predictions added to stacking test dataset...')\n",
    "    print(\"-\"*50)\n",
    "    names.pop(0)\n",
    "    \n",
    "    row_index+=1\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "361abc03-5db0-40f2-b1b7-368520ccdc60",
    "_uuid": "36b21d674a02eb9f3679f7dd956c8cc8ba36ea5b"
   },
   "source": [
    "- Let's take a quick look at what these new datasets look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0cc617cc-5f6f-4987-865b-30bdbbaa5428",
    "_uuid": "d23c743e525c0602b3c9184e3ef996e75f4eae43",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stacked_validation_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "28bdfb89-774a-4b88-9195-081900a907cf",
    "_uuid": "c90da674cb6f8b70db07c201456b2c8a72d73717"
   },
   "source": [
    "- The new training dataset is 438 rows of predictions from the 8 algorithms we decided to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bfd4cca5-6f63-423a-8d5b-ecc675c827da",
    "_uuid": "9316e775ab7ab9d248d641f9392c1e9da2caf959",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stacked_test_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4a093e47-fc34-4dae-8817-2736e9201472",
    "_uuid": "3f26c46d19c0f3a5ace41424bd1c6aa5bd0d6257"
   },
   "source": [
    "- The new test dataset is 1459 rows of predictions from the 8 algorithms we decided to use.\n",
    "- I will use these two datasets to train and produce predictions for the meta-model, Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a83da925-5728-4f30-b36e-57fd0c9a66d5",
    "_uuid": "f4830270c8ab7b0c34f245bbd49cc406bc6770d5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First drop the Lasso results from the table, as we will be using Lasso as the meta-model\n",
    "drop = ['Lasso']\n",
    "stacked_validation_train.drop(drop, axis=1, inplace=True)\n",
    "stacked_test_train.drop(drop, axis=1, inplace=True)\n",
    "\n",
    "# Now fit the meta model and generate predictions\n",
    "meta_model = make_pipeline(RobustScaler(), Lasso(alpha=0.00001, copy_X = True, fit_intercept = True,\n",
    "                                              normalize = False, precompute = False, max_iter = 10000,\n",
    "                                              tol = 0.0001, selection = 'random', random_state = None))\n",
    "meta_model.fit(stacked_validation_train, Y_test)\n",
    "\n",
    "meta_model_pred = np.expm1(meta_model.predict(stacked_test_train))\n",
    "print(\"Meta-model trained and applied!...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "51746316-3cb9-4b7e-b55e-cd5d384c4435",
    "_uuid": "222405ac40ba2db577caae15eb2c169122b5c963"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "41605cea-75bc-4e19-981e-b09cd81aebd2",
    "_uuid": "ad157b089382a8689cf070ead37bebf67d2be0d6"
   },
   "source": [
    "### 5.5 - Ensemble\n",
    "\n",
    "- However, another famous and successful technique for Machine Learning are **Ensemble methods**.\n",
    "    - These are effective when using many different models of varying degrees of accuracy. \n",
    "    - They work on the idea that many weak learners, can produce a strong learner.\n",
    "- Therefore, using the meta-model that I will create, I will also combine this with the results of the individual optimised models to create an ensemble.\n",
    "- In order to create this ensemble, I must collect the final predictions of each of the optimised models. I will do this now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ba90cb16-c442-4e74-9647-688b9103ac1d",
    "_uuid": "cb28d78b15723ad96116209c320cd1792e498655",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [KernelRidge(), ElasticNet(), Lasso(), GradientBoostingRegressor(), BayesianRidge(), LassoLarsIC(), RandomForestRegressor(), xgb.XGBRegressor()]\n",
    "names = ['KernelRidge', 'ElasticNet', 'Lasso', 'Gradient Boosting', 'Bayesian Ridge', 'Lasso Lars IC', 'Random Forest', 'XGBoost']\n",
    "params_grid = [KR_param_grid, EN_param_grid, LASS_param_grid, GB_param_grid, BR_param_grid, LL_param_grid, RFR_param_grid, XGB_param_grid]\n",
    "final_predictions = pd.DataFrame()\n",
    "\n",
    "row_index=0\n",
    "\n",
    "for alg in models:\n",
    "    \n",
    "    gs_alg = GridSearchCV(alg, param_grid = params_grid[0], cv = shuff, scoring = 'neg_mean_squared_error', n_jobs=-1)\n",
    "    params_grid.pop(0)\n",
    "    \n",
    "    gs_alg.fit(stacked_validation_train, Y_test)\n",
    "    gs_best = gs_alg.best_estimator_\n",
    "    final_predictions.insert(loc = row_index, column = names[0], value = np.expm1(gs_best.predict(stacked_test_train)))\n",
    "    print(row_index+1, alg.__class__.__name__, 'final results predicted added to table...')\n",
    "    names.pop(0)\n",
    "    \n",
    "    row_index+=1\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"Done\")\n",
    "    \n",
    "final_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "50322298-1835-4d18-8e6b-44357a189366",
    "_uuid": "484c1eb67169d8c989016661e919455c185277e7"
   },
   "source": [
    "- As you can see, each of the models produces results that vary quite widely. This is the beauty of using a combination of many different models.\n",
    "- Some models will be much better at catching certain signals in the data, whereas others may perform better in other situations. \n",
    "- By creating an ensemble of all of these results, it helps to create a more generalised model that is resistant to noise.\n",
    "- Now, I will finish by creating an ensemble of the meta-model and optimised models, for my final submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "489645cd-db29-4b28-83dc-d386fda427aa",
    "_uuid": "7c0b7bb4d34cf39ee5fa468b0e539295c9a535dd"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8ada214a-232f-4063-bfed-613c89936f33",
    "_uuid": "f9bbf1deefbc7d3c6548fe2a5f56e02bf0205221"
   },
   "source": [
    "### 5.6 - Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "18e75b1c-4e26-4f0d-a393-97e1fd4a7786",
    "_uuid": "e1f77cf2a818f9feb3a4e6a3aae441b56d615ae7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble = meta_model_pred*(1/10) + final_predictions['XGBoost']*(1.5/10) + final_predictions['Gradient Boosting']*(2/10) + final_predictions['Bayesian Ridge']*(1/10) + final_predictions['Lasso']*(1/10) + final_predictions['KernelRidge']*(1/10) + final_predictions['Lasso Lars IC']*(1/10) + final_predictions['Random Forest']*(1.5/10)\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['Id'] = test_ID\n",
    "submission['SalePrice'] = ensemble\n",
    "submission.to_csv('final_submission.csv',index=False)\n",
    "print(\"Submission file, created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "628cb63c-0c6e-4b55-a8ec-f3ced85fe79f",
    "_uuid": "1023828a1ee15e1e01a48be7f4a88d4db51ce14a"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "adf0b044-a22b-4148-b0f3-43b54fe3e60c",
    "_uuid": "b491a12762b0c9a681d6fe75173851fff3ed45a1"
   },
   "source": [
    "# 6. \n",
    "## Conclusion\n",
    "\n",
    "- Throughout this notebook, I wanted to focus mainly on **feature engineering** and the **stacking** technique. I think stacking is a very useful tool to have within your Data Science toolkit, and I hope this has helped you to understand how it works.\n",
    "- This is just my solution, but I'd be interested to hear your comments and thoughts on my work and also how you'd do it differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6a3abeab-ac70-4b19-8d4e-5ed6cdeb1067",
    "_uuid": "dbb4fbdd24277ad97362d5131c2f3a4953fe9baf"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "75bd2576-3a62-48e1-bf44-059535b4e441",
    "_uuid": "2ca5e6c19489c91029142e3488c50718c1918620",
    "collapsed": true
   },
   "source": [
    "## Acknowledgements\n",
    "\n",
    "- The Ames Housing dataset, by Dean De Cock: https://ww2.amstat.org/publications/jse/v19n3/decock.pdf\n",
    "- Curve fitting with linear and nonlinear regression: http://blog.minitab.com/blog/adventures-in-statistics-2/curve-fitting-with-linear-and-nonlinear-regression\n",
    "- Stacking: https://www.coursera.org/learn/competitive-data-science/lecture/Qdtt6/stacking\n",
    "\n",
    "**Useful Kernels**:\n",
    "- Juliencs: https://www.kaggle.com/juliencs/a-study-on-regression-applied-to-the-ames-dataset\n",
    "- Serigne: https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard\n",
    "- Alexandru Papiu: https://www.kaggle.com/apapiu/regularized-linear-models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
